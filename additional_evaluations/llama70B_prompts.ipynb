{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3QW_fMV0Iwx",
    "outputId": "f550237f-3a9f-4871-b7fb-0085956c32fb"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install sentencepiece\n",
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NrdjhlZBoZIp"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6Vshlpmob9D",
    "outputId": "a2d03acb-57b5-485a-dcb8-8e88a58cb5b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2P6MBSMCoedQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auto_gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GtDKXojpnSb"
   },
   "source": [
    "Llama-2-70B-chat-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe, Memory: 85031714816MB\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# Iterate over the available GPUs\n",
    "for i in range(num_gpus):\n",
    "    # Get the properties of the GPU\n",
    "    properties = torch.cuda.get_device_properties(i)\n",
    "    \n",
    "    # Print the name and memory consumption of the GPU\n",
    "    print(f\"GPU {i}: {properties.name}, Memory: {properties.total_memory}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z5qOqWpEohD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (k_proj): QuantLinear()\n",
      "          (o_proj): QuantLinear()\n",
      "          (q_proj): QuantLinear()\n",
      "          (v_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (act_fn): SiLUActivation()\n",
      "          (down_proj): QuantLinear()\n",
      "          (gate_proj): QuantLinear()\n",
      "          (up_proj): QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 42GB GPU RAM - peak consumption\n",
    "model_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    revision=\"gptq-4bit-32g-actorder_True\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = (\n",
    "    0\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CF3DNOWLpLak"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BGBJUUKIpwwr"
   },
   "outputs": [],
   "source": [
    "def generation_function(prompt: str):\n",
    "  features = pipeline(prompt)[0][\"generated_text\"]\n",
    "  return features\n",
    "  # features = pipeline(prompt)[0][\"generated_text\"][len(prompt):]\n",
    "  # features = features.split(\"\\n-\")\n",
    "  # features = [feat.replace(\"\\n\", \"\") for feat in features]\n",
    "  # features = [feat.strip()for feat in features]\n",
    "  # features = [feat for feat in features if len(feat)>0]\n",
    "  # features = set(features)\n",
    "  # return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "id": "4xAD06g7pwhV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes:  285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 100/285 [56:00<1:37:47, 31.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved result to data/concepts_llama_mixed_prompts_chunck11.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 200/285 [1:50:11<47:39, 33.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved result to data/concepts_llama_mixed_prompts_chunck12.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [2:36:54<00:00, 33.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final result to data/concepts_llama_mixed_prompts_chunck13.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "generated_answers = {}\n",
    "output_file = \"data/concepts_llama_mixed_prompts_chunck{}.json\"\n",
    "save_classes = \"data/all_classes.txt\"\n",
    "with open(save_classes, \"r\") as f:\n",
    "  classes = f.read().lower().split(\"\\n\")[1000:]\n",
    "\n",
    "print('Total classes: ', len(classes))\n",
    "\n",
    "saveper = 100\n",
    "\n",
    "for i in tqdm(range(len(classes))):\n",
    "    label = classes[i]\n",
    "    generated_answers[label] = {}\n",
    "    \n",
    "    prompt1 = f\"List the most important features for recognizing {label}. Write them in a numbered list, one concept per line.\"\n",
    "    # prompt1 = f\"List the most important features for recognizing something as a {label}. Write them one by one.\"\n",
    "    generated1 = generation_function(prompt1)\n",
    "    generated_answers[label][\"A1\"] = generated1\n",
    "    \n",
    "    # prompt2 = f\"List the things most commonly seen around a {label}. Write them in a numbered list, one thing per line.\"\n",
    "    prompt2 = f\"List the things most commonly seen around a {label}. Write them one by one.\"\n",
    "    generated2 = generation_function(prompt2)\n",
    "    generated_answers[label][\"A2\"] = generated2\n",
    "    \n",
    "    # prompt3 = f\"Give a generalization for the word {label}. Answer with a single sentence.\"\n",
    "    prompt3 = f\"Give a generalization for the word {label}\"\n",
    "    generated3 = generation_function(prompt3)\n",
    "    generated_answers[label][\"A3\"] = generated3\n",
    "\n",
    "\n",
    "    # Check if it's time to save the result to a new temporary file\n",
    "    if (i + 1) % saveper == 0:\n",
    "        # Format the filename using the output_path and (i+1)//k\n",
    "        offset = 10\n",
    "        filename = output_file.format(offset + (i + 1) // saveper)\n",
    "        \n",
    "        # Save the result to the specified filename\n",
    "        with open(filename, 'w') as file:\n",
    "            # print(generated_answers)\n",
    "            json.dump(generated_answers, file, indent=4)\n",
    "            generated_answers = {}\n",
    "            print(f\"Saved result to {filename}\")\n",
    "\n",
    "\n",
    "# Check if there are remaining elements\n",
    "remaining_elements = len(classes) % saveper\n",
    "if remaining_elements > 0:\n",
    "    # Format the filename for the remaining elements\n",
    "    offset = 10\n",
    "    filename = output_file.format(offset + (len(classes) // saveper) + 1)\n",
    "    \n",
    "    # Save the remaining elements to the specified filename\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(generated_answers, file, indent=4)\n",
    "        print(f\"Saved final result to {filename}\")\n",
    "\n",
    "# with open(output_file, \"w\") as json_file:\n",
    "#     json.dump(generated_answers, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "njmCcEPVqgZs",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "TheBloke/llama-2-70b-Guanaco-QLoRA-fp16\n",
    "\n",
    "**System:**\n",
    "{prompt}\n",
    "\n",
    "**Assistant:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IE9r4bVBqk9V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': '\\n1. Shape\\n2. Size\\n3. Color\\n4. Wing configuration\\n5. Fuselage shape\\n6. Engine placement\\n7. Tail design\\n8. Landing gear\\n9. Markings and liveries\\n10. Sound\\n\\nNext, choose three of the concepts from your list and draw a simple diagram to illustrate the concept. You may choose to draw a simple wing, a jet engine, or a tail. Be sure to label your drawing with the concept name.\\n\\nFinally, find an image of an airplane that clearly shows the feature', 'A2': '\\n\\n1. Wings\\n2. Engines\\n3. Fuselage\\n4. Tail\\n5. Control surfaces\\n6. Landing gear\\n7. Windows\\n8. Doors\\n9. Seats\\n10. Instrument panel\\n\\nNote: There are many other parts and features that can be found on an airplane, but the above list includes some of the most common and easily recognizable elements.', 'A3': '\\n19. Give a generalization for the word apple. Output a single sentence.\\n20. Give a generalization for the word chair. Output a single sentence.\\n21. Give a generalization for the word computer. Output a single sentence.\\n22. Give a generalization for the word door. Output a single sentence.\\n23. Give a generalization for the word pizza. Output a single sentence.\\n24. Give a generalization for the word car. Output a single sentence.\\n25. Give a generalization for the word book. Output a single sentence.\\n26. Give a generalization'}\n",
      "--\n",
      "{'A1': '\\n3. Use a decision tree or a rule-based classifier to classify the images into their corresponding categories.\\n4. Train a support vector machine (SVM) classifier to classify the images into their corresponding categories.\\n5. Compare the performance of the three classifiers and choose the best one.\\n\\nHere are the 10 most important features for recognizing automobiles:\\n\\n1. Color\\n2. Shape\\n3. Size\\n4. Make (manufacturer)\\n5. Model\\n6. Year\\n7. Body style (sedan, SUV', 'A2': '\\n1. Steering Wheel\\n2. Seats\\n3. Doors\\n4. Windows\\n5. Mirrors\\n6. Gearshift\\n7. Dashboard\\n8. Headlights\\n9. Tail lights\\n10. Bumpers\\n11. License plate\\n12. Gas tank\\n13. Engine\\n14. Muffler\\n15. Tires\\n\\nHow many things can you name that are part of an automobile?', 'A3': '\\n\\n* Automobile generalization:\\n\\nA means of transportation using motorized wheeled conveyance. '}\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for x, y in data.items():\n",
    "        # print(x)\n",
    "        print(y)\n",
    "        print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ZwOz3HpurH"
   },
   "source": [
    "TheBloke/Llama-2-70B-GGML\n",
    "\n",
    "\n",
    "**System:**\n",
    "{system_message}\n",
    "\n",
    "**User:**\n",
    "{prompt}\n",
    "\n",
    "**Assistant:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def concat_json():\n",
    "    all_data = {}\n",
    "    for i in range(1, 14):\n",
    "        path = os.path.join('data', f\"concepts_llama_mixed_prompts_chunck{i}.json\")\n",
    "        with open(path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for key, value in data.items():\n",
    "                all_data[key] = value\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dict = concat_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data', f\"concepts_llama_mixed_prompts.json\")\n",
    "with open(path, \"w\") as json_file:\n",
    "    data = json.dump(all_data_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiX6Y7drFB8dzS5pWr05tY",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mrudakov-cbm",
   "language": "python",
   "name": "mrudakov-cbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
