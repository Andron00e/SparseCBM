{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 18:03:23.661726: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 18:03:23.661811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 18:03:23.661842: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 18:03:23.670663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 18:03:24.615730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../bottlenecks\")\n",
    "import configs\n",
    "from cbm import *\n",
    "from data_utils import *\n",
    "from trainer_utils import *\n",
    "from graph_plot_tools import *\n",
    "from utils import *\n",
    "from metric_utils import *\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from typing import List, Dict, Optional\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "configs.set_seed(42)\n",
    "device = configs.set_device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-PCIE-40GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cub_filtered.txt\", \"r\") as f:\n",
    "    concepts = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cub_classes.txt\", \"r\") as f:\n",
    "    class_names = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black footed Albatross',\n",
       " 'Laysan Albatross',\n",
       " 'Sooty Albatross',\n",
       " 'Groove billed Ani',\n",
       " 'Crested Auklet',\n",
       " 'Least Auklet',\n",
       " 'Parakeet Auklet',\n",
       " 'Rhinoceros Auklet',\n",
       " 'Brewer Blackbird',\n",
       " 'Red winged Blackbird']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [c.lower() for c in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6dc4cea90041d596df0ae01b929590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader_preprocessed  = prepared_dataloaders(Constants.cub200_link,\n",
    "                                                  concepts=class_names,\n",
    "                                                  prep_loaders=\"train\",\n",
    "                                                  batch_size=128,\n",
    "                                                  backbone_name=Constants.clip_large_link,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred)).contiguous()\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zeroshoting_backbone_model(model, loader, precision_metric, recall_metric, f1_metric):\n",
    "    top_1_accs, top_5_accs = [], []\n",
    "    top_1_precisions, top_1_recalls = [], []\n",
    "    top_1_f1scores = []\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in tqdm(enumerate(loader, 0)):\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            inputs, labels = batch\n",
    "            inputs, targets = inputs.to(device), torch.LongTensor(labels).to(\n",
    "                device\n",
    "            )\n",
    "            logits_per_image = model(**inputs).logits_per_image\n",
    "            top_1, top_5 = accuracy(logits_per_image, targets, topk=(1, 5))\n",
    "            top_1_accs.append(top_1.detach().cpu().item())\n",
    "            top_5_accs.append(top_5.detach().cpu().item())\n",
    "\n",
    "            precs = precision_metric.compute(\n",
    "                    predictions=logits_per_image.argmax(dim=-1).cpu(),\n",
    "                    references=targets.cpu(),\n",
    "                    average=\"weighted\",\n",
    "                )\n",
    "            recs = recall_metric.compute(\n",
    "                    predictions=logits_per_image.argmax(dim=-1).cpu(),\n",
    "                    references=targets.cpu(),\n",
    "                    average=\"weighted\",\n",
    "                )\n",
    "            f1 = f1_metric.compute(\n",
    "                    predictions=logits_per_image.argmax(dim=-1).cpu(),\n",
    "                    references=targets.cpu(),\n",
    "                    average=\"weighted\",\n",
    "                    labels=np.unique(logits_per_image.argmax(dim=-1).cpu()),\n",
    "                )\n",
    "            \n",
    "            top_1_precisions.append(precs[\"precision\"])\n",
    "            top_1_recalls.append(recs[\"recall\"])\n",
    "            top_1_f1scores.append(f1[\"f1\"])\n",
    "            \n",
    "    return (\n",
    "        np.mean(top_1_accs),\n",
    "        np.mean(top_5_accs),\n",
    "        np.mean(top_1_precisions),\n",
    "        np.mean(top_1_recalls),\n",
    "        np.mean(top_1_f1scores),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "model = transformers.CLIPModel.from_pretrained(Constants.clip_large_link)\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 427,616,513 || all params: 427,616,513 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accs, top_5_accs, \\\n",
    "top_1_precisions, top_1_recalls, \\\n",
    "top_1_f1scores = zeroshoting_backbone_model(model, \n",
    "                                        train_loader_preprocessed,\n",
    "                                        precision_metric,\n",
    "                                        recall_metric,\n",
    "                                        f1_metric,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.63503688090557"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.32546347540779"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6396276499973812"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6263503692646135"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8121238334459757"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_f1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cifar10_classes.txt\", \"r\") as f:\n",
    "    class_names = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6da25e257f34760b8c4db483ba589ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader_preprocessed  = prepared_dataloaders(Constants.cifar10_link,\n",
    "                                                  concepts=class_names,\n",
    "                                                  prep_loaders=\"train\",\n",
    "                                                  batch_size=32,\n",
    "                                                  backbone_name=Constants.clip_large_link,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accs, top_5_accs, \\\n",
    "top_1_precisions, top_1_recalls, \\\n",
    "top_1_f1scores = zeroshoting_backbone_model(model, \n",
    "                                        train_loader_preprocessed,\n",
    "                                        precision_metric,\n",
    "                                        recall_metric,\n",
    "                                        f1_metric,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy:  81.79166666666667 \n",
      "\n",
      "Top 5 Accuracy:  97.51041666666667 \n",
      "\n",
      "Top 1 Precision:  0.8665994243025493 \n",
      "\n",
      "Top 1 Recall:  0.8179166666666666 \n",
      "\n",
      "F1:  0.8257207012040896 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 1 Accuracy: \", top_1_accs, \"\\n\")\n",
    "print(\"Top 5 Accuracy: \", top_5_accs, \"\\n\")\n",
    "print(\"Top 1 Precision: \", top_1_precisions, \"\\n\")\n",
    "print(\"Top 1 Recall: \", top_1_recalls, \"\\n\")\n",
    "print(\"F1: \", top_1_f1scores, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cifar100_classes.txt\", \"r\") as f:\n",
    "    class_names = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fda077a1b2475ab4f1f19a4484d1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77e548638194fb98a8ec41ed26f3cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/27.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd869cafe75844b298452d361e5bf9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c7ccd9f0b74dcaa72bd6909455a770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader_preprocessed  = prepared_dataloaders(Constants.cifar100_link,\n",
    "                                                  concepts=class_names,\n",
    "                                                  prep_loaders=\"train\",\n",
    "                                                  batch_size=32,\n",
    "                                                  backbone_name=Constants.clip_large_link,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accs, top_5_accs, \\\n",
    "top_1_precisions, top_1_recalls, \\\n",
    "top_1_f1scores = zeroshoting_backbone_model(model, \n",
    "                                        train_loader_preprocessed,\n",
    "                                        precision_metric,\n",
    "                                        recall_metric,\n",
    "                                        f1_metric,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy:  52.84166666666667 \n",
      "\n",
      "Top 5 Accuracy:  76.825 \n",
      "\n",
      "Top 1 Precision:  0.5473391293891294 \n",
      "\n",
      "Top 1 Recall:  0.5284166666666666 \n",
      "\n",
      "F1:  0.8452402787050571 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 1 Accuracy: \", top_1_accs, \"\\n\")\n",
    "print(\"Top 5 Accuracy: \", top_5_accs, \"\\n\")\n",
    "print(\"Top 1 Precision: \", top_1_precisions, \"\\n\")\n",
    "print(\"Top 1 Recall: \", top_1_recalls, \"\\n\")\n",
    "print(\"F1: \", top_1_f1scores, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andronserv",
   "language": "python",
   "name": "andronserv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
